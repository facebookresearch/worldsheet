model_config:
  mesh_renderer:
    batch_size: ${training.batch_size}
    image_size_H: 256
    image_size_W: 256
    grid_stride: 8
    z_min: 0.02
    z_max: 10.
    pred_inv_z: true
    pred_inv_z_synsin: false
    backbone_name: resnet50
    backbone_dim: 2048
    backbone_lr: 1e-4  # might be too high; kept for backward compatibility
    rendering:
      blur_radius: 1e-6
      faces_per_pixel: 10
      clip_barycentric_coords: false
      hfov: 90
      sigma: 1e-4
      gamma: 1e-4
      z_background: 1.7432  # Replica mean depth
      background_color: [0.64926944, 0.61896299, 0.64431339]  # Replica mean RGB
      gblur_kernel_size: 7
      gblur_sigma: 2
      gblur_weight_thresh: 1e-4
    train_z_grid_only: false
    mesh_laplacian_use_l2_loss: true
    loss_weights:
      z_grid_l1_0: 200
      depth_l1_0: 200
      depth_l1_1: 200
      image_l1_1: 400
      vgg19_perceptual_1: 0.  # kept for backward compatibility
      grid_offset: 20
      mesh_laplacian: 0.01
      image_l1_1_inpaint: 0.  # kept for backward compatibility
      vgg19_perceptual_1_inpaint: 0.  # kept for backward compatibility
    save_forward_results: false
    forward_results_dir: save/visualization
    return_rendering_results_only: false
    fill_z_with_gt: false
    # the inpainting network
    use_inpainting: false
    sanity_check_inpaint_with_gt: false
    inpainting:
      net_G:
        ngf: 32
        n_downsampling: 3  # pix2pixHD uses 4 downsampling layers in global genenator; but since we have smaller input size, let's just use 3 here
        n_blocks: 9
        norm: instance
        use_alpha_input: false  # kept for backward compatibility