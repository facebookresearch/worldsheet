optimizer:
  type: adam_w  # HuggingFace transformer's AdamW
  params:
    lr: 1e-4
    eps: 1e-8
    weight_decay: 1e-5

scheduler:
  type: multi_step
  params:
    use_warmup: true
    lr_steps:
    - 25000
    - 28000
    lr_ratio: 0.2
    warmup_iterations: 500
    warmup_factor: 0.25

training:
  num_workers: 2
  clip_norm_mode: all
  clip_gradients: true
  max_grad_l2_norm: 5
  lr_scheduler: true
  max_updates: 30000
  checkpoint_interval: 5000
  evaluation_interval: 5000
  log_interval: 20
  batch_size: 64
  early_stop:
    enabled: false
    criteria: total_loss
    minimize: true
  stdout_capture: false
  find_unused_parameters: true
