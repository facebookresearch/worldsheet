{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /private/home/ronghanghu/workspace/mmf_nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import argparse\n",
    "import omegaconf\n",
    "import matplotlib.pyplot as plt\n",
    "import quaternion\n",
    "from tqdm import tqdm\n",
    "\n",
    "from mmf.utils.env import setup_imports\n",
    "from mmf.utils.configuration import Configuration\n",
    "from mmf.utils.build import build_config, build_model\n",
    "from mmf.common.sample import SampleList, Sample\n",
    "\n",
    "\n",
    "def get_config_from_opts(opts):\n",
    "    setup_imports()\n",
    "\n",
    "    args = argparse.Namespace(config_override=None)\n",
    "    args.opts = opts\n",
    "\n",
    "    configuration = Configuration(args)\n",
    "    config = build_config(configuration)\n",
    "    return config\n",
    "\n",
    "\n",
    "def load_model(config, device, ckpt_file=None):\n",
    "    attributes = config.model_config[config.model]\n",
    "    # Easy way to point to config for other model\n",
    "    if isinstance(attributes, str):\n",
    "        attributes = config.model_config[attributes]\n",
    "\n",
    "    with omegaconf.open_dict(attributes):\n",
    "        attributes.model = config.model\n",
    "\n",
    "    model = build_model(attributes)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    if ckpt_file is not None:\n",
    "        state_dict = torch.load(ckpt_file, map_location=device)[\"model\"]\n",
    "        try:\n",
    "            model.load_state_dict(state_dict, strict=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('retry loading with `strict=False`')\n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_sample_list(img_0, depth_0, R_0, T_0, R_1, T_1, image_transform):\n",
    "    sample = Sample()\n",
    "    sample.orig_img_0 = torch.tensor(img_0)\n",
    "    sample.depth_0 = torch.tensor(depth_0)\n",
    "    sample.trans_img_0 = image_transform(sample.orig_img_0.permute((2, 0, 1)))\n",
    "    sample.R_0 = torch.tensor(R_0)\n",
    "    sample.T_0 = torch.tensor(T_0)\n",
    "    sample.R_1 = torch.tensor(R_1)\n",
    "    sample.T_1 = torch.tensor(T_1)\n",
    "    sample_list = SampleList([sample]).to(device)\n",
    "    return sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_habitat_position_rotation(R, T):\n",
    "    P = np.eye(4, dtype=np.float32)\n",
    "    P[0:3, 0:3] = R.T\n",
    "    P[0:3, 3] = T\n",
    "    \n",
    "    # change from Habitat coordinates to PyTorch3D coordinates\n",
    "    P[0] *= -1  # flip X axis\n",
    "    P[2] *= -1  # flip Z axis\n",
    "    \n",
    "    Pinv = np.linalg.inv(P)\n",
    "    position = Pinv[0:3, 3]\n",
    "    rotation = Pinv[0:3, 0:3]\n",
    "    rotation = quaternion.from_rotation_matrix(rotation)\n",
    "    \n",
    "    return position, rotation\n",
    "\n",
    "    \n",
    "def get_pytorch3d_camera_RT(position, rotation):\n",
    "    rotation = quaternion.as_rotation_matrix(rotation)\n",
    "\n",
    "    Pinv = np.eye(4, dtype=np.float32)\n",
    "    Pinv[0:3, 0:3] = rotation\n",
    "    Pinv[0:3, 3] = position\n",
    "    P = np.linalg.inv(Pinv)\n",
    "\n",
    "    # change from Habitat coordinates to PyTorch3D coordinates\n",
    "    P[0] *= -1  # flip X axis\n",
    "    P[2] *= -1  # flip Z axis\n",
    "\n",
    "    R = P[0:3, 0:3].T  # to row major\n",
    "    T = P[0:3, 3]\n",
    "\n",
    "    return R, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_camera(R_in, T_in, degree_right):\n",
    "    position_in, rotation_in = get_habitat_position_rotation(R_in, T_in)\n",
    "    angle = -degree_right * np.pi / 180\n",
    "\n",
    "    horizontal_rotation = quaternion.from_float_array(\n",
    "        [np.cos(angle), 0, np.sin(angle), 0]\n",
    "    )  # wxyz-format\n",
    "    rotation_out = horizontal_rotation * rotation_in\n",
    "    R_out, T_out = get_pytorch3d_camera_RT(position_in, rotation_out)\n",
    "    return R_out, T_out\n",
    "\n",
    "\n",
    "def move_camera(R_in, T_in, front, right, distance):\n",
    "    position_in, rotation_in = get_habitat_position_rotation(R_in, T_in)\n",
    "\n",
    "    # transform direction vector from camera to world\n",
    "    direction_vec = np.array([right, 0, -front], np.float32)\n",
    "    direction_vec = quaternion.as_rotation_matrix(rotation_in) @ direction_vec\n",
    "\n",
    "    # remove motion along Y axis (vertical) and re-normalize\n",
    "    direction_vec[1] = 0.\n",
    "    direction_vec = direction_vec / np.linalg.norm(direction_vec)\n",
    "\n",
    "    position_out = position_in + direction_vec * distance\n",
    "    R_out, T_out = get_pytorch3d_camera_RT(position_out, rotation_in)\n",
    "    return R_out, T_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"depth\"\n",
    "split = \"val\"\n",
    "\n",
    "opts = [\n",
    "    f\"config=projects/neural_rendering/configs/diode/{exp_name}.yaml\",\n",
    "    f\"datasets=diode\",\n",
    "    f\"model=mesh_renderer\",\n",
    "    f\"training.batch_size=1\",\n",
    "    f\"model_config.mesh_renderer.return_rendering_results_only=True\",\n",
    "#     f\"model_config.mesh_renderer.fill_z_with_gt=True\",\n",
    "    f\"model_config.mesh_renderer.grid_stride=4\",\n",
    "]\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "ckpt_file = f\"./save/diode/{exp_name}/best.ckpt\"\n",
    "if not os.path.exists(ckpt_file):\n",
    "    ckpt_file = ckpt_file.replace(\"best.ckpt\", \"current.ckpt\")\n",
    "assert os.path.exists(ckpt_file)\n",
    "\n",
    "config = get_config_from_opts(opts)\n",
    "model = load_model(config, device, ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('/private/home/ronghanghu/workspace/DATASETS/diode-devkit/')\n",
    "\n",
    "import diode\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import torch\n",
    "import skimage.transform\n",
    "\n",
    "\n",
    "class DiodeProcessor:\n",
    "    def __init__(self):\n",
    "        self.CROP_W = 734\n",
    "        self.OUT_SIZE = 256\n",
    "\n",
    "    def crop_and_resize(self, im, order):\n",
    "        H, W = im.shape[:2]\n",
    "        diff = (W - self.CROP_W) // 2\n",
    "        im = im[:, diff:-diff]\n",
    "        out = skimage.transform.resize(im, (self.OUT_SIZE, self.OUT_SIZE), order=order)\n",
    "        return out\n",
    "    \n",
    "    def __call__(self, im, de, de_mask):\n",
    "        de = de.copy()\n",
    "        de[de_mask == 0] = 0\n",
    "        im = self.crop_and_resize(im, order=None)\n",
    "        de = self.crop_and_resize(de, order=0)  # nearest neighbor sampling on depth map\n",
    "        de = de.astype(np.float32)\n",
    "        \n",
    "        # also downsample the mask and use it to mask invalid regions\n",
    "        de_mask = self.crop_and_resize(de_mask, order=None)\n",
    "        de_mask = (1 - de_mask < 1e-8)\n",
    "        de[~de_mask] = 0\n",
    "        return im, de\n",
    "\n",
    "\n",
    "dataset = diode.DIODE(\n",
    "    meta_fname='/private/home/ronghanghu/workspace/DATASETS/diode-devkit/diode_meta.json',\n",
    "    data_root='/checkpoint/ronghanghu/neural_rendering_datasets/diode/',\n",
    "    splits=['val'],\n",
    "    scene_types=['outdoor']\n",
    ")\n",
    "\n",
    "processor = DiodeProcessor()\n",
    "\n",
    "idx = 103\n",
    "im, de, de_mask = dataset[idx]\n",
    "im_out, de_out = processor(im, de, de_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved_data_file = f\"/checkpoint/ronghanghu/neural_rendering_datasets/replica/{split}/data/{scene}.npz\"\n",
    "# saved_results_file = f'./save/visualization/{exp_name}/{split}/{scene}_outputs.npz'\n",
    "\n",
    "# d = np.load(saved_data_file)\n",
    "# data = dict(d)\n",
    "# d.close()\n",
    "# d = np.load(saved_results_file)\n",
    "# data.update(d)\n",
    "# d.close()\n",
    "\n",
    "# normalize with ResNet-50 preprocessing\n",
    "image_transform = torchvision.transforms.Normalize(\n",
    "    [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_init = np.eye(3, dtype=np.float32)\n",
    "T_init = np.zeros(3, dtype=np.float32)\n",
    "\n",
    "data = {\n",
    "    'orig_img_0': im_out.astype(np.float32),\n",
    "    'depth_0': de_out,\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(11, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(im_out)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(np.log(de_out))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_pic(r, t, return_torch_tensors=False, show=True):\n",
    "    sample_list = build_sample_list(\n",
    "        img_0=data['orig_img_0'],\n",
    "        depth_0=data['depth_0'],\n",
    "        R_0=R_init,\n",
    "        T_0=T_init,\n",
    "        R_1=r,\n",
    "        T_1=t,\n",
    "        image_transform=image_transform\n",
    "    ).to(device)\n",
    "\n",
    "    if return_torch_tensors:\n",
    "        rendering_results = model.forward(sample_list)\n",
    "        rgba_out = rendering_results['rgba_out_rec_list'][1][0, ..., :3]\n",
    "        depth_out = rendering_results['depth_out_rec_list'][1][0]\n",
    "        return rgba_out, depth_out\n",
    "\n",
    "    with torch.no_grad():\n",
    "        rendering_results = model.forward(sample_list)\n",
    "\n",
    "    rgba_out = rendering_results['rgba_out_rec_list'][1][0, ..., :3].cpu().numpy()\n",
    "    rgba_out = np.clip(rgba_out, 0, 1)\n",
    "    depth_out = rendering_results['depth_out_rec_list'][1][0].cpu().numpy()\n",
    "    if show:\n",
    "        plt.figure(figsize=(11, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(rgba_out[..., :3])\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(np.log(depth_out))\n",
    "        plt.colorbar()\n",
    "\n",
    "    return rgba_out\n",
    "\n",
    "\n",
    "def rotate(R, T, angles, cameras, sampling=3):\n",
    "    if not isinstance(angles, list):\n",
    "        angles = [angles]\n",
    "    for n, e in enumerate(angles):\n",
    "        b = angles[n-1] if n > 0 else 0\n",
    "        for a in np.linspace(b, e, int(np.abs(e-b)*sampling)):\n",
    "            R_new, T_new = rotate_camera(R, T, a)\n",
    "            cameras.append((R_new, T_new))\n",
    "\n",
    "    return R_new, T_new\n",
    "\n",
    "\n",
    "def move(R, T, distances, cameras, sampling=6):\n",
    "    if not isinstance(distances, list):\n",
    "        distances = [distances]\n",
    "    for n, e in enumerate(distances):\n",
    "        b = distances[n-1] if n > 0 else 0\n",
    "        for d in np.linspace(b, e, int(np.abs(e-b)*sampling)):\n",
    "            R_new, T_new = move_camera(R, T, 1, 0, d)\n",
    "            cameras.append((R_new, T_new))\n",
    "\n",
    "    return R_new, T_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d_gt = torch.tensor(de_out, device=device)\n",
    "d_mask = d_gt.gt(0.1).float()\n",
    "\n",
    "solver = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "for n_iter in range(40):\n",
    "    solver.zero_grad()\n",
    "    _, d = take_pic(R_init, T_init, return_torch_tensors=True, show=False)\n",
    "    loss = torch.abs((d - d_gt) * d_mask).sum()\n",
    "    print(loss.item())\n",
    "    loss.backward()\n",
    "    solver.step()\n",
    "\n",
    "#     if n_iter % 5 == 0:\n",
    "#         take_pic(R_init, T_init)\n",
    "#         plt.title(f'iter {n_iter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "cameras = []\n",
    "R_new, T_new = R_init, T_init\n",
    "\n",
    "for _ in range(30):\n",
    "    cameras.append((R_new, T_new))\n",
    "\n",
    "take_pic(R_new, T_new)\n",
    "R_new, T_new = move(R_new, T_new, 5, cameras); take_pic(R_new, T_new)\n",
    "R_new, T_new = rotate(R_new, T_new, -20, cameras); take_pic(R_new, T_new)\n",
    "R_new, T_new = move(R_new, T_new, 5, cameras); take_pic(R_new, T_new)\n",
    "R_new, T_new = rotate(R_new, T_new, 60, cameras); take_pic(R_new, T_new)\n",
    "R_new, T_new = rotate(R_new, T_new, -60, cameras); take_pic(R_new, T_new)\n",
    "R_new, T_new = move(R_new, T_new, -5, cameras); take_pic(R_new, T_new)\n",
    "R_new, T_new = rotate(R_new, T_new, 20, cameras); take_pic(R_new, T_new)\n",
    "R_new, T_new = move(R_new, T_new, -5, cameras); take_pic(R_new, T_new)\n",
    "R_new, T_new = rotate(R_new, T_new, [30, -20, 0], cameras); take_pic(R_new, T_new)\n",
    "\n",
    "for _ in range(30):\n",
    "    cameras.append((R_new, T_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "for r, t in tqdm(cameras):\n",
    "    rgba_out = take_pic(r, t, show=False)\n",
    "\n",
    "    # float32 -> uint8, RGB -> BGR\n",
    "    frames.append(skimage.img_as_ubyte(rgba_out[..., ::-1]))\n",
    "\n",
    "video_file = f\"/private/home/ronghanghu/workspace/gtdepth65x65fit_depth_diode_{idx:04d}.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"MP4V\")\n",
    "fps = 30\n",
    "frame_size = (256, 256)\n",
    "writer = cv2.VideoWriter(video_file, fourcc, fps, frame_size)\n",
    "\n",
    "for img in frames:\n",
    "    writer.write(img)\n",
    "writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
