{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /private/home/ronghanghu/workspace/mmf_nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import argparse\n",
    "import omegaconf\n",
    "import matplotlib.pyplot as plt\n",
    "import quaternion\n",
    "from tqdm import tqdm\n",
    "\n",
    "from mmf.utils.env import setup_imports\n",
    "from mmf.utils.configuration import Configuration\n",
    "from mmf.utils.build import build_config, build_model\n",
    "from mmf.common.sample import SampleList, Sample\n",
    "\n",
    "\n",
    "def get_config_from_opts(opts):\n",
    "    setup_imports()\n",
    "\n",
    "    args = argparse.Namespace(config_override=None)\n",
    "    args.opts = opts\n",
    "\n",
    "    configuration = Configuration(args)\n",
    "    config = build_config(configuration)\n",
    "    return config\n",
    "\n",
    "\n",
    "def load_model(config, device, ckpt_file=None):\n",
    "    attributes = config.model_config[config.model]\n",
    "    # Easy way to point to config for other model\n",
    "    if isinstance(attributes, str):\n",
    "        attributes = config.model_config[attributes]\n",
    "\n",
    "    with omegaconf.open_dict(attributes):\n",
    "        attributes.model = config.model\n",
    "\n",
    "    model = build_model(attributes)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    if ckpt_file is not None:\n",
    "        state_dict = torch.load(ckpt_file, map_location=device)[\"model\"]\n",
    "        try:\n",
    "            model.load_state_dict(state_dict, strict=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('retry loading with `strict=False`')\n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_sample_list(img_0, R_0, T_0, R_1, T_1, image_transform):\n",
    "    sample = Sample()\n",
    "    sample.orig_img_0 = torch.tensor(img_0)\n",
    "    sample.trans_img_0 = image_transform(sample.orig_img_0.permute((2, 0, 1)))\n",
    "    sample.R_0 = torch.tensor(R_0)\n",
    "    sample.T_0 = torch.tensor(T_0)\n",
    "    sample.R_1 = torch.tensor(R_1)\n",
    "    sample.T_1 = torch.tensor(T_1)\n",
    "    sample_list = SampleList([sample]).to(device)\n",
    "    return sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_habitat_position_rotation(R, T):\n",
    "    P = np.eye(4, dtype=np.float32)\n",
    "    P[0:3, 0:3] = R.T\n",
    "    P[0:3, 3] = T\n",
    "    \n",
    "    # change from Habitat coordinates to PyTorch3D coordinates\n",
    "    P[0] *= -1  # flip X axis\n",
    "    P[2] *= -1  # flip Z axis\n",
    "    \n",
    "    Pinv = np.linalg.inv(P)\n",
    "    position = Pinv[0:3, 3]\n",
    "    rotation = Pinv[0:3, 0:3]\n",
    "    rotation = quaternion.from_rotation_matrix(rotation)\n",
    "    \n",
    "    return position, rotation\n",
    "\n",
    "    \n",
    "def get_pytorch3d_camera_RT(position, rotation):\n",
    "    rotation = quaternion.as_rotation_matrix(rotation)\n",
    "\n",
    "    Pinv = np.eye(4, dtype=np.float32)\n",
    "    Pinv[0:3, 0:3] = rotation\n",
    "    Pinv[0:3, 3] = position\n",
    "    P = np.linalg.inv(Pinv)\n",
    "\n",
    "    # change from Habitat coordinates to PyTorch3D coordinates\n",
    "    P[0] *= -1  # flip X axis\n",
    "    P[2] *= -1  # flip Z axis\n",
    "\n",
    "    R = P[0:3, 0:3].T  # to row major\n",
    "    T = P[0:3, 3]\n",
    "\n",
    "    return R, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_camera(R_in, T_in, degree_right):\n",
    "    position_in, rotation_in = get_habitat_position_rotation(R_in, T_in)\n",
    "    angle = -degree_right * np.pi / 180\n",
    "\n",
    "    horizontal_rotation = quaternion.from_float_array(\n",
    "        [np.cos(angle), 0, np.sin(angle), 0]\n",
    "    )  # wxyz-format\n",
    "    rotation_out = horizontal_rotation * rotation_in\n",
    "    R_out, T_out = get_pytorch3d_camera_RT(position_in, rotation_out)\n",
    "    return R_out, T_out\n",
    "\n",
    "\n",
    "def move_camera(R_in, T_in, front, right, distance):\n",
    "    position_in, rotation_in = get_habitat_position_rotation(R_in, T_in)\n",
    "\n",
    "    # transform direction vector from camera to world\n",
    "    direction_vec = np.array([right, 0, -front], np.float32)\n",
    "    direction_vec = quaternion.as_rotation_matrix(rotation_in) @ direction_vec\n",
    "\n",
    "    # remove motion along Y axis (vertical) and re-normalize\n",
    "    direction_vec[1] = 0.\n",
    "    direction_vec = direction_vec / np.linalg.norm(direction_vec)\n",
    "\n",
    "    position_out = position_in + direction_vec * distance\n",
    "    R_out, T_out = get_pytorch3d_camera_RT(position_out, rotation_in)\n",
    "    return R_out, T_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"only_image_l1_loss_perceptual_l1laplacian_largebl\"\n",
    "split = \"train\"\n",
    "scene = \"scene_0002_sample_00000400\"\n",
    "\n",
    "opts = [\n",
    "    f\"config=projects/neural_rendering/configs/replica/{exp_name}.yaml\",\n",
    "    f\"datasets=replica\",\n",
    "    f\"model=mesh_renderer\",\n",
    "    f\"training.batch_size=1\",\n",
    "    f\"model_config.mesh_renderer.return_rendering_results_only=True\",\n",
    "]\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "ckpt_file = f\"./save/replica/{exp_name}/best.ckpt\"\n",
    "if not os.path.exists(ckpt_file):\n",
    "    ckpt_file = ckpt_file.replace(\"best.ckpt\", \"current.ckpt\")\n",
    "assert os.path.exists(ckpt_file)\n",
    "\n",
    "config = get_config_from_opts(opts)\n",
    "model = load_model(config, device, ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_data_file = f\"/checkpoint/ronghanghu/neural_rendering_datasets/replica/{split}/data/{scene}.npz\"\n",
    "saved_results_file = f'./save/visualization/{exp_name}/{split}/{scene}_outputs.npz'\n",
    "\n",
    "d = np.load(saved_data_file)\n",
    "data = dict(d)\n",
    "d.close()\n",
    "d = np.load(saved_results_file)\n",
    "data.update(d)\n",
    "d.close()\n",
    "\n",
    "# normalize with ResNet-50 preprocessing\n",
    "image_transform = torchvision.transforms.Normalize(\n",
    "    [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_init = data[\"camera_Rs\"][0]\n",
    "T_init = data[\"camera_Ts\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_pic(r, t, show=True):\n",
    "    sample_list = build_sample_list(\n",
    "        img_0=data['orig_img_0'],\n",
    "        R_0=R_init,\n",
    "        T_0=T_init,\n",
    "        R_1=r,\n",
    "        T_1=t,\n",
    "        image_transform=image_transform\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        rendering_results = model.forward(sample_list)\n",
    "\n",
    "    rgba_out = rendering_results['rgba_out_rec_list'][1][0, ..., :3].cpu().numpy()\n",
    "    rgba_out = np.clip(rgba_out, 0, 1)\n",
    "    if show:\n",
    "        plt.figure()\n",
    "        plt.imshow(rgba_out[..., :3])\n",
    "    return rgba_out\n",
    "\n",
    "\n",
    "def rotate(R, T, angles, cameras, sampling=2):\n",
    "    if not isinstance(angles, list):\n",
    "        angles = [angles]\n",
    "    for n, e in enumerate(angles):\n",
    "        b = angles[n-1] if n > 0 else 0\n",
    "        for a in np.linspace(b, e, int(np.abs(e-b)*sampling)):\n",
    "            R_new, T_new = rotate_camera(R, T, a)\n",
    "            cameras.append((R_new, T_new))\n",
    "\n",
    "    return R_new, T_new\n",
    "\n",
    "\n",
    "def move(R, T, distances, cameras, sampling=30):\n",
    "    if not isinstance(distances, list):\n",
    "        distances = [distances]\n",
    "    for n, e in enumerate(distances):\n",
    "        b = distances[n-1] if n > 0 else 0\n",
    "        for d in np.linspace(b, e, int(np.abs(e-b)*sampling)):\n",
    "            R_new, T_new = move_camera(R, T, 1, 0, d)\n",
    "            cameras.append((R_new, T_new))\n",
    "\n",
    "    return R_new, T_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "cameras = []\n",
    "R_new, T_new = R_init, T_init\n",
    "\n",
    "for _ in range(30):\n",
    "    cameras.append((R_new, T_new))\n",
    "\n",
    "take_pic(R_new, T_new)\n",
    "R_new, T_new = rotate(R_new, T_new, 10, cameras); take_pic(R_new, T_new)\n",
    "R_new, T_new = move(R_new, T_new, [3, 1.5], cameras); take_pic(R_new, T_new)\n",
    "R_new, T_new = rotate(R_new, T_new, [30, -45, 0], cameras); take_pic(R_new, T_new)\n",
    "R_new, T_new = move(R_new, T_new, -1.5, cameras); take_pic(R_new, T_new)\n",
    "R_new, T_new = rotate(R_new, T_new, [30, -30, -10], cameras); take_pic(R_new, T_new)\n",
    "\n",
    "for _ in range(30):\n",
    "    cameras.append((R_new, T_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "for r, t in tqdm(cameras):\n",
    "    rgba_out = take_pic(r, t, show=False)\n",
    "\n",
    "    # float32 -> uint8, RGB -> BGR\n",
    "    frames.append(skimage.img_as_ubyte(rgba_out[..., ::-1]))\n",
    "\n",
    "video_file = f\"/private/home/ronghanghu/workspace/{exp_name}_{split}_{scene}.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"MP4V\")\n",
    "fps = 30\n",
    "frame_size = (256, 256)\n",
    "writer = cv2.VideoWriter(video_file, fourcc, fps, frame_size)\n",
    "\n",
    "for img in frames:\n",
    "    writer.write(img)\n",
    "writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
