{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /private/home/ronghanghu/workspace/mmf_nr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import argparse\n",
    "import omegaconf\n",
    "import matplotlib.pyplot as plt\n",
    "import quaternion\n",
    "from tqdm import tqdm\n",
    "from skimage import img_as_float, img_as_ubyte\n",
    "from PIL import Image\n",
    "\n",
    "from mmf.utils.env import setup_imports\n",
    "from mmf.utils.configuration import Configuration\n",
    "from mmf.utils.build import build_config, build_model\n",
    "from mmf.common.sample import SampleList, Sample\n",
    "\n",
    "\n",
    "def get_config_from_opts(opts):\n",
    "    setup_imports()\n",
    "\n",
    "    args = argparse.Namespace(config_override=None)\n",
    "    args.opts = opts\n",
    "\n",
    "    configuration = Configuration(args)\n",
    "    config = build_config(configuration)\n",
    "    return config\n",
    "\n",
    "\n",
    "def load_model(config, device, ckpt_file=None):\n",
    "    attributes = config.model_config[config.model]\n",
    "    # Easy way to point to config for other model\n",
    "    if isinstance(attributes, str):\n",
    "        attributes = config.model_config[attributes]\n",
    "\n",
    "    with omegaconf.open_dict(attributes):\n",
    "        attributes.model = config.model\n",
    "\n",
    "    model = build_model(attributes)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    if ckpt_file is not None:\n",
    "        state_dict = torch.load(ckpt_file, map_location=device)[\"model\"]\n",
    "        try:\n",
    "            model.load_state_dict(state_dict, strict=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('retry loading with `strict=False`')\n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_sample_list(img_0, R_0, T_0, R_1, T_1, image_transform):\n",
    "    sample = Sample()\n",
    "    sample.orig_img_0 = torch.tensor(img_0)\n",
    "    sample.trans_img_0 = image_transform(sample.orig_img_0.permute((2, 0, 1)))\n",
    "    sample.R_0 = torch.tensor(R_0)\n",
    "    sample.T_0 = torch.tensor(T_0)\n",
    "    sample.R_1 = torch.tensor(R_1)\n",
    "    sample.T_1 = torch.tensor(T_1)\n",
    "    sample_list = SampleList([sample]).to(device)\n",
    "    return sample_list\n",
    "\n",
    "\n",
    "exp_name = \"realestate10k_dscale2_stride4ft_lowerL1_200\"\n",
    "ckpt_name = \"models/model_40000.ckpt\"\n",
    "\n",
    "opts = [\n",
    "    f\"config=projects/neural_rendering/configs/synsin_realestate10k/{exp_name}.yaml\",\n",
    "    f\"datasets=synsin_realestate10k\",\n",
    "    f\"model=mesh_renderer\",\n",
    "    f\"training.batch_size=1\",\n",
    "    f\"model_config.mesh_renderer.return_rendering_results_only=True\",\n",
    "]\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "ckpt_file = f\"./save/synsin_realestate10k/{exp_name}/{ckpt_name}\"\n",
    "assert os.path.exists(ckpt_file)\n",
    "\n",
    "config = get_config_from_opts(opts)\n",
    "model = load_model(config, device, ckpt_file)\n",
    "\n",
    "\n",
    "image_size = 256\n",
    "frame_dir = \"/checkpoint/ronghanghu/neural_rendering_datasets/realestate10K/RealEstate10K/all_frames/test/\"\n",
    "# normalize with ResNet-50 preprocessing\n",
    "image_transform = torchvision.transforms.Normalize(\n",
    "    [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "\n",
    "def load_image(image_path, W):\n",
    "    image = img_as_float(\n",
    "        np.array(Image.open(image_path).resize([W, W], Image.BILINEAR))\n",
    "    ).astype(np.float32)\n",
    "    return image\n",
    "\n",
    "\n",
    "def build_realestate10k_RT_from_txt(intrinsics, extrinsics):\n",
    "    offset = np.array([[2, 0, -1], [0, -2, 1], [0, 0, -1]], dtype=np.float32)\n",
    "\n",
    "    intrinsics = np.array(intrinsics, dtype=np.float32).reshape(4)\n",
    "    extrinsics = np.array(extrinsics, dtype=np.float32).reshape(3, 4)\n",
    "\n",
    "    origK = np.array(\n",
    "        [\n",
    "            [intrinsics[0], 0, intrinsics[2]],\n",
    "            [0, intrinsics[1], intrinsics[3]],\n",
    "            [0, 0, 1],\n",
    "        ],\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "    K = np.matmul(offset, origK)\n",
    "    P = np.matmul(K, extrinsics)\n",
    "\n",
    "    # map to PyTorch3d coordinates\n",
    "    P = P.copy()\n",
    "    # change from Habitat coordinates to PyTorch3D coordinates\n",
    "    P[0] *= -1  # flip X axis\n",
    "    P[2] *= -1  # flip Z axis\n",
    "    R = P[0:3, 0:3].T  # to row major\n",
    "    T = P[0:3, 3]\n",
    "    \n",
    "    return R, T\n",
    "\n",
    "\n",
    "def take_pic_ours(img_init, intrinsics, extrinsics_init, extrinsics_new, use_inpainting=False, show=True):\n",
    "    R_0, T_0 = build_realestate10k_RT_from_txt(intrinsics, extrinsics_init)\n",
    "    R_1, T_1 = build_realestate10k_RT_from_txt(intrinsics, extrinsics_new)\n",
    "    sample_list = build_sample_list(\n",
    "        img_0=img_init,\n",
    "        R_0=R_0,\n",
    "        T_0=T_0,\n",
    "        R_1=R_1,\n",
    "        T_1=T_1,\n",
    "        image_transform=image_transform\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        rendering_results = model.forward(sample_list)\n",
    "\n",
    "    out = rendering_results['rgb_1_inpaint'] if use_inpainting else rendering_results['rgba_out_rec_list'][1]\n",
    "    rgba_out = out[0, ..., :3].cpu().numpy()\n",
    "    rgba_out = np.clip(rgba_out, 0, 1)\n",
    "    if show:\n",
    "        plt.figure()\n",
    "        plt.imshow(rgba_out[..., :3])\n",
    "    return rgba_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SynSin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import quaternion\n",
    "import numpy as np\n",
    "\n",
    "import sys; sys.path.append(\"/private/home/ronghanghu/workspace/synsin/\")\n",
    "import os; os.environ['DEBUG'] = ''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from skimage import img_as_float\n",
    "from PIL import Image\n",
    "\n",
    "from models.networks.sync_batchnorm import convert_model\n",
    "from models.base_model import BaseModel\n",
    "from options.options import get_model\n",
    "\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "# REALESTATE\n",
    "SYNSIN_MODEL_PATH = '/private/home/ronghanghu/workspace/synsin/modelcheckpoints/realestate/synsin.pth'\n",
    "synsin_opts = torch.load(SYNSIN_MODEL_PATH)['opts']\n",
    "synsin_model = get_model(synsin_opts)\n",
    "torch_devices = [int(gpu_id.strip()) for gpu_id in synsin_opts.gpu_ids.split(\",\")]\n",
    "\n",
    "if 'sync' in synsin_opts.norm_G:\n",
    "    synsin_model = convert_model(synsin_model)\n",
    "    synsin_model = nn.DataParallel(synsin_model, torch_devices[0:1]).cuda()\n",
    "else:\n",
    "    synsin_model = nn.DataParallel(synsin_model, torch_devices[0:1]).cuda()\n",
    "\n",
    "#  Load the original model to be tested\n",
    "synsin_model_to_test = BaseModel(synsin_model, synsin_opts)\n",
    "synsin_model_to_test.load_state_dict(torch.load(SYNSIN_MODEL_PATH)['state_dict'])\n",
    "synsin_model_to_test.eval()\n",
    "\n",
    "print(\"Loaded model\")\n",
    "\n",
    "\n",
    "# image_size = 256\n",
    "# frame_dir = \"/checkpoint/ronghanghu/neural_rendering_datasets/realestate10K/RealEstate10K/all_frames/test/\"\n",
    "synsin_input_transform = torchvision.transforms.Normalize(\n",
    "    (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    ")\n",
    "\n",
    "\n",
    "def load_image(image_path, W):\n",
    "    image = img_as_float(\n",
    "        np.array(Image.open(image_path).resize([W, W], Image.BILINEAR))\n",
    "    ).astype(np.float32)\n",
    "    return image\n",
    "\n",
    "\n",
    "def build_synsin_realestate10k_batch(intrinsics, extrinsics_src, extrinsics_tgt, src_image, tgt_image):\n",
    "    offset = np.array([[2, 0, -1], [0, -2, 1], [0, 0, -1]], dtype=np.float32)\n",
    "\n",
    "    intrinsics = np.array(intrinsics, dtype=np.float32).reshape(4)\n",
    "    src_pose = extrinsics_src.reshape(3, 4)\n",
    "    tgt_pose = extrinsics_tgt.reshape(3, 4)\n",
    "\n",
    "    src_image = synsin_input_transform(torch.tensor(src_image).permute((2, 0, 1)))\n",
    "    tgt_image = synsin_input_transform(torch.tensor(tgt_image).permute((2, 0, 1)))\n",
    "\n",
    "    poses = [src_pose, tgt_pose]\n",
    "    cameras = []\n",
    "\n",
    "    for pose in poses:\n",
    "\n",
    "        origK = np.array(\n",
    "            [\n",
    "                [intrinsics[0], 0, intrinsics[2]],\n",
    "                [0, intrinsics[1], intrinsics[3]],\n",
    "                [0, 0, 1],\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        K = np.matmul(offset, origK)\n",
    "\n",
    "        P = pose\n",
    "        P = np.matmul(K, P)\n",
    "        # Merge these together to match habitat\n",
    "        P = np.vstack((P, np.zeros((1, 4)))).astype(np.float32)\n",
    "        P[3, 3] = 1\n",
    "\n",
    "        # Now artificially flip x/ys to match habitat\n",
    "        Pinv = np.linalg.inv(P)\n",
    "\n",
    "        cameras += [{\n",
    "            \"P\": torch.tensor(P).unsqueeze(0),\n",
    "            \"Pinv\": torch.tensor(Pinv).unsqueeze(0),\n",
    "            \"K\": torch.eye(4, dtype=torch.float32).unsqueeze(0),\n",
    "            \"Kinv\": torch.eye(4, dtype=torch.float32).unsqueeze(0)\n",
    "        }]\n",
    "        \n",
    "    images = torch.stack([\n",
    "        src_image.unsqueeze(0),\n",
    "        tgt_image.unsqueeze(0)\n",
    "    ])\n",
    "\n",
    "    return {\"images\": images, \"cameras\": cameras}\n",
    "\n",
    "\n",
    "def take_synsin_pic(img_init, intrinsics, extrinsics_init, extrinsics_new, show=True):\n",
    "    batch = build_synsin_realestate10k_batch(intrinsics, extrinsics_init, extrinsics_new, img_init, img_init)\n",
    "    iter_data_loader = iter([batch])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, pred_imgs, batch = synsin_model_to_test(\n",
    "            iter_data_loader, isval=True, return_batch=True\n",
    "        )\n",
    "\n",
    "    rgb_out = pred_imgs[\"PredImg\"][0].detach().cpu().numpy().transpose(1, 2, 0)\n",
    "    rgb_out = np.clip(rgb_out, 0, 1)\n",
    "    if show:\n",
    "        plt.figure()\n",
    "        plt.imshow(rgb_out)\n",
    "    return rgb_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Im2Im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IM2IM_MODEL_PATH = '/private/home/ronghanghu/workspace/synsin/modelcheckpoints/realestate/viewappearance.pth'\n",
    "im2im_opts = torch.load(IM2IM_MODEL_PATH)['opts']\n",
    "im2im_model = get_model(im2im_opts)\n",
    "torch_devices = [int(gpu_id.strip()) for gpu_id in im2im_opts.gpu_ids.split(\",\")]\n",
    "\n",
    "if 'sync' in im2im_opts.norm_G:\n",
    "    im2im_model = convert_model(im2im_model)\n",
    "    im2im_model = nn.DataParallel(im2im_model, torch_devices[0:1]).cuda()\n",
    "else:\n",
    "    im2im_model = nn.DataParallel(im2im_model, torch_devices[0:1]).cuda()\n",
    "\n",
    "#  Load the original model to be tested\n",
    "im2im_model_to_test = BaseModel(im2im_model, im2im_opts)\n",
    "im2im_model_to_test.load_state_dict(torch.load(IM2IM_MODEL_PATH)['state_dict'])\n",
    "im2im_model_to_test.eval()\n",
    "\n",
    "\n",
    "def take_im2im_pic(img_init, intrinsics, extrinsics_init, extrinsics_new, show=True):\n",
    "    batch = build_synsin_realestate10k_batch(intrinsics, extrinsics_init, extrinsics_new, img_init, img_init)\n",
    "    iter_data_loader = iter([batch])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, pred_imgs, batch = im2im_model_to_test(\n",
    "            iter_data_loader, isval=True, return_batch=True\n",
    "        )\n",
    "\n",
    "    rgb_out = pred_imgs[\"PredImg\"][0].detach().cpu().numpy().transpose(1, 2, 0)\n",
    "    rgb_out = np.clip(rgb_out, 0, 1)\n",
    "    if show:\n",
    "        plt.figure()\n",
    "        plt.imshow(rgb_out)\n",
    "    return rgb_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera transform functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrinsics2RT_orig(extrinsics):\n",
    "    extrinsics = np.array(extrinsics, dtype=np.float32).reshape(3, 4)\n",
    "\n",
    "    # map to PyTorch3d coordinates\n",
    "    P_orig = extrinsics.copy()\n",
    "    # change from Habitat coordinates to PyTorch3D coordinates\n",
    "    P_orig[0] *= -1  # flip X axis\n",
    "    P_orig[2] *= -1  # flip Z axis\n",
    "    R_orig = P_orig[0:3, 0:3].T  # to row major\n",
    "    T_orig = P_orig[0:3, 3]\n",
    "    \n",
    "    _check_valid_rotation_matrix(R_orig)\n",
    "    return R_orig, T_orig\n",
    "\n",
    "\n",
    "def RT_orig2extrinsics(R_orig, T_orig):\n",
    "    _check_valid_rotation_matrix(R_orig)\n",
    "\n",
    "    P_orig = np.hstack((R_orig.T, T_orig[..., None]))\n",
    "\n",
    "    P_orig = P_orig.copy()\n",
    "    P_orig[0] *= -1  # flip X axis\n",
    "    P_orig[2] *= -1  # flip Z axis\n",
    "    extrinsics = P_orig.reshape(-1)\n",
    "    return extrinsics\n",
    "\n",
    "\n",
    "def _check_valid_rotation_matrix(R, tol: float = 1e-4):\n",
    "    \"\"\"\n",
    "    Determine if R is a valid rotation matrix by checking it satisfies the\n",
    "    following conditions:\n",
    "    ``RR^T = I and det(R) = 1``\n",
    "    Args:\n",
    "        R: an (N, 3, 3) matrix\n",
    "    Returns:\n",
    "        None\n",
    "    Emits a warning if R is an invalid rotation matrix.\n",
    "    \"\"\"\n",
    "    eye = np.eye(3)\n",
    "    orthogonal = np.all(np.abs(R @ R.T - eye) < tol)\n",
    "    no_distortion = np.abs(np.linalg.det(R) - 1) < tol\n",
    "\n",
    "    if not (orthogonal and no_distortion):\n",
    "        raise Exception(\"R is not a valid rotation matrix\")\n",
    "    return\n",
    "\n",
    "\n",
    "def _get_habitat_position_rotation(R, T):\n",
    "    P = np.eye(4, dtype=np.float32)\n",
    "    P[0:3, 0:3] = R.T\n",
    "    P[0:3, 3] = T\n",
    "    \n",
    "    # change from Habitat coordinates to PyTorch3D coordinates\n",
    "    P[0] *= -1  # flip X axis\n",
    "    P[2] *= -1  # flip Z axis\n",
    "    \n",
    "    Pinv = np.linalg.inv(P)\n",
    "    position = Pinv[0:3, 3]\n",
    "    rotation = Pinv[0:3, 0:3]\n",
    "    rotation = quaternion.from_rotation_matrix(rotation)\n",
    "    \n",
    "    return position, rotation\n",
    "\n",
    "    \n",
    "def _get_pytorch3d_camera_RT(position, rotation):\n",
    "    rotation = quaternion.as_rotation_matrix(rotation)\n",
    "\n",
    "    Pinv = np.eye(4, dtype=np.float32)\n",
    "    Pinv[0:3, 0:3] = rotation\n",
    "    Pinv[0:3, 3] = position\n",
    "    P = np.linalg.inv(Pinv)\n",
    "\n",
    "    # change from Habitat coordinates to PyTorch3D coordinates\n",
    "    P[0] *= -1  # flip X axis\n",
    "    P[2] *= -1  # flip Z axis\n",
    "\n",
    "    R = P[0:3, 0:3].T  # to row major\n",
    "    T = P[0:3, 3]\n",
    "\n",
    "    return R, T\n",
    "\n",
    "\n",
    "def _h_rotate_camera(R_in, T_in, degree_right):\n",
    "    position_in, rotation_in = _get_habitat_position_rotation(R_in, T_in)\n",
    "    angle = -degree_right * np.pi / 180\n",
    "\n",
    "    horizontal_rotation = quaternion.from_float_array(\n",
    "        [np.cos(angle), 0, np.sin(angle), 0]\n",
    "    )  # wxyz-format\n",
    "    rotation_out = horizontal_rotation * rotation_in\n",
    "    R_out, T_out = _get_pytorch3d_camera_RT(position_in, rotation_out)\n",
    "    return R_out, T_out\n",
    "\n",
    "\n",
    "def _v_rotate_camera(R_in, T_in, degree_up):\n",
    "    position_in, rotation_in = _get_habitat_position_rotation(R_in, T_in)\n",
    "    angle = -degree_up * np.pi / 180\n",
    "\n",
    "    horizontal_rotation = quaternion.from_float_array(\n",
    "        [np.cos(angle), np.sin(angle), 0, 0]\n",
    "    )  # wxyz-format\n",
    "    rotation_out = horizontal_rotation * rotation_in\n",
    "    R_out, T_out = _get_pytorch3d_camera_RT(position_in, rotation_out)\n",
    "    return R_out, T_out\n",
    "\n",
    "\n",
    "def _i_rotate_camera(R_in, T_in, degree_in):\n",
    "    position_in, rotation_in = _get_habitat_position_rotation(R_in, T_in)\n",
    "    angle = -degree_in * np.pi / 180\n",
    "\n",
    "    horizontal_rotation = quaternion.from_float_array(\n",
    "        [np.cos(angle), 0, 0, np.sin(angle)]\n",
    "    )  # wxyz-format\n",
    "    # unlike _h_rotate_camera or _v_rotate_camera\n",
    "    # here we multiply horizontal_rotation after rotation_in\n",
    "    rotation_out = rotation_in * horizontal_rotation\n",
    "    R_out, T_out = _get_pytorch3d_camera_RT(position_in, rotation_out)\n",
    "    return R_out, T_out\n",
    "\n",
    "\n",
    "def _move_camera(R_in, T_in, front, up, right, distance):\n",
    "    position_in, rotation_in = _get_habitat_position_rotation(R_in, T_in)\n",
    "\n",
    "    # transform direction vector from camera to world\n",
    "    direction_vec = np.array([right, up, -front], np.float32)\n",
    "    direction_vec = quaternion.as_rotation_matrix(rotation_in) @ direction_vec\n",
    "\n",
    "    position_out = position_in + direction_vec * distance\n",
    "    R_out, T_out = _get_pytorch3d_camera_RT(position_out, rotation_in)\n",
    "    return R_out, T_out\n",
    "\n",
    "\n",
    "def _normalize(array):\n",
    "    return array / np.sqrt(np.sum(array * array))\n",
    "\n",
    "\n",
    "def _interpolate(ra, rb, theta, t):\n",
    "    if abs(theta) < 1e-3:\n",
    "        # the angle is too small, 1 / sin(theta) will be numerically unstable\n",
    "        return _normalize((1-t) * ra + t * rb)\n",
    "    else:\n",
    "        return _normalize((np.sin((1-t) * theta) * ra + np.sin(t * theta) * rb)  / np.sin(theta))\n",
    "\n",
    "\n",
    "def _interpolate_rotation_quaternion(rotation_a, rotation_b, num):\n",
    "    # https://stackoverflow.com/questions/4099369/interpolate-between-rotation-matrices\n",
    "    ra = quaternion.as_float_array(rotation_a)\n",
    "    rb = quaternion.as_float_array(rotation_b)\n",
    "    dot = np.dot(ra, rb)\n",
    "    if dot < 0:\n",
    "        dot = -dot\n",
    "        rb = -rb\n",
    "    dot = np.clip(dot, -1, 1)\n",
    "    theta = np.arccos(dot)\n",
    "\n",
    "    interpolated_rotations = [\n",
    "        quaternion.from_float_array(_interpolate(ra, rb, theta, t))\n",
    "        for t in np.linspace(0, 1, num)\n",
    "    ]\n",
    "    return interpolated_rotations\n",
    "\n",
    "\n",
    "def _extrapolate_rotation_quaternion(rotation_a, rotation_b, ratio, num):\n",
    "    # https://stackoverflow.com/questions/4099369/interpolate-between-rotation-matrices\n",
    "    ra = quaternion.as_float_array(rotation_a)\n",
    "    rb = quaternion.as_float_array(rotation_b)\n",
    "    dot = np.dot(ra, rb)\n",
    "    if dot < 0:\n",
    "        dot = -dot\n",
    "        rb = -rb\n",
    "    dot = np.clip(dot, -1, 1)\n",
    "    theta = np.arccos(dot)\n",
    "\n",
    "    interpolated_rotations = [\n",
    "        quaternion.from_float_array(_interpolate(ra, rb, theta, t))\n",
    "        for t in np.linspace(1, 1+ratio, num)\n",
    "    ]\n",
    "    return interpolated_rotations\n",
    "\n",
    "\n",
    "def h_rotate(R, T, angles, cameras, sampling=3):\n",
    "    if not isinstance(angles, list):\n",
    "        angles = [angles]\n",
    "    for n, e in enumerate(angles):\n",
    "        b = angles[n-1] if n > 0 else 0\n",
    "        for a in np.linspace(b, e,  max(2, int(np.abs(e-b)*sampling))):\n",
    "            R_new, T_new = _h_rotate_camera(R, T, a)\n",
    "            cameras.append((R_new, T_new))\n",
    "\n",
    "    return R_new, T_new\n",
    "\n",
    "\n",
    "def v_rotate(R, T, angles, cameras, sampling=3):\n",
    "    if not isinstance(angles, list):\n",
    "        angles = [angles]\n",
    "    for n, e in enumerate(angles):\n",
    "        b = angles[n-1] if n > 0 else 0\n",
    "        for a in np.linspace(b, e,  max(2, int(np.abs(e-b)*sampling))):\n",
    "            R_new, T_new = _v_rotate_camera(R, T, a)\n",
    "            cameras.append((R_new, T_new))\n",
    "\n",
    "    return R_new, T_new\n",
    "\n",
    "\n",
    "def i_rotate(R, T, angles, cameras, sampling=3):\n",
    "    if not isinstance(angles, list):\n",
    "        angles = [angles]\n",
    "    for n, e in enumerate(angles):\n",
    "        b = angles[n-1] if n > 0 else 0\n",
    "        for a in np.linspace(b, e,  max(2, int(np.abs(e-b)*sampling))):\n",
    "            R_new, T_new = _i_rotate_camera(R, T, a)\n",
    "            cameras.append((R_new, T_new))\n",
    "\n",
    "    return R_new, T_new\n",
    "\n",
    "\n",
    "def move(R, T, distances, FUR_ratios, cameras, sampling=20):\n",
    "    fwd_ratio, up_ratio, right_ratio = FUR_ratios\n",
    "    if not isinstance(distances, list):\n",
    "        distances = [distances]\n",
    "    for n, e in enumerate(distances):\n",
    "        b = distances[n-1] if n > 0 else 0\n",
    "        for d in np.linspace(b, e, max(2, int(np.abs(e-b)*sampling))):\n",
    "            R_new, T_new = _move_camera(R, T, fwd_ratio, up_ratio, right_ratio, d)\n",
    "            cameras.append((R_new, T_new))\n",
    "\n",
    "    return R_new, T_new\n",
    "\n",
    "\n",
    "def interpolate_RTs(R_a, T_a, R_b, T_b, cameras, num):\n",
    "    position_a, rotation_a = _get_habitat_position_rotation(R_a, T_a)\n",
    "    position_b, rotation_b = _get_habitat_position_rotation(R_b, T_b)\n",
    "    positions = np.linspace(position_a, position_b, num)\n",
    "    rotations = _interpolate_rotation_quaternion(rotation_a, rotation_b, num)\n",
    "\n",
    "    Rs, Ts = zip(*(_get_pytorch3d_camera_RT(p, r) for p, r in zip(positions, rotations)))\n",
    "    for R_new, T_new in zip(Rs, Ts):\n",
    "        cameras.append((R_new, T_new))\n",
    "\n",
    "    return R_new, T_new\n",
    "\n",
    "\n",
    "def extrapolate_RTs(R_a, T_a, R_b, T_b, ratio, cameras, num):\n",
    "    position_a, rotation_a = _get_habitat_position_rotation(R_a, T_a)\n",
    "    position_b, rotation_b = _get_habitat_position_rotation(R_b, T_b)\n",
    "    positions = np.linspace(position_b, position_b + (position_b - position_a) * ratio, num)\n",
    "    rotations = _extrapolate_rotation_quaternion(rotation_a, rotation_b, ratio, num)\n",
    "\n",
    "    Rs, Ts = zip(*(_get_pytorch3d_camera_RT(p, r) for p, r in zip(positions, rotations)))\n",
    "    for R_new, T_new in zip(Rs, Ts):\n",
    "        cameras.append((R_new, T_new))\n",
    "\n",
    "    return R_new, T_new\n",
    "\n",
    "\n",
    "def extrinsics_h_rotate(extrinsics, angles, extrinsics_cameras, sampling=3):\n",
    "    R, T = extrinsics2RT_orig(extrinsics)\n",
    "    cameras = []\n",
    "    h_rotate(R, T, angles, cameras, sampling)\n",
    "    extrinsics_cameras.extend(RT_orig2extrinsics(r, t) for r, t in cameras)\n",
    "    return extrinsics_cameras[-1]\n",
    "\n",
    "\n",
    "def extrinsics_v_rotate(extrinsics, angles, extrinsics_cameras, sampling=3):\n",
    "    R, T = extrinsics2RT_orig(extrinsics)\n",
    "    cameras = []\n",
    "    v_rotate(R, T, angles, cameras, sampling)\n",
    "    extrinsics_cameras.extend(RT_orig2extrinsics(r, t) for r, t in cameras)\n",
    "    return extrinsics_cameras[-1]\n",
    "\n",
    "\n",
    "def extrinsics_i_rotate(extrinsics, angles, extrinsics_cameras, sampling=3):\n",
    "    R, T = extrinsics2RT_orig(extrinsics)\n",
    "    cameras = []\n",
    "    i_rotate(R, T, angles, cameras, sampling)\n",
    "    extrinsics_cameras.extend(RT_orig2extrinsics(r, t) for r, t in cameras)\n",
    "    return extrinsics_cameras[-1]\n",
    "\n",
    "\n",
    "def extrinsics_move(extrinsics, distances, FUR_ratios, extrinsics_cameras, sampling=20):\n",
    "    R, T = extrinsics2RT_orig(extrinsics)\n",
    "    cameras = []\n",
    "    move(R, T, distances, FUR_ratios, cameras, sampling)\n",
    "    extrinsics_cameras.extend(RT_orig2extrinsics(r, t) for r, t in cameras)\n",
    "    return extrinsics_cameras[-1]\n",
    "\n",
    "\n",
    "def extrinsics_interpolate_RTs(extrinsics_a, extrinsics_b, extrinsics_cameras, num):\n",
    "    R_a, T_a = extrinsics2RT_orig(extrinsics_a)\n",
    "    R_b, T_b = extrinsics2RT_orig(extrinsics_b)\n",
    "    cameras = []\n",
    "    interpolate_RTs(R_a, T_a, R_b, T_b, cameras, num)\n",
    "    extrinsics_cameras.extend(RT_orig2extrinsics(r, t) for r, t in cameras)\n",
    "    return extrinsics_cameras[-1]\n",
    "\n",
    "\n",
    "def extrinsics_extrapolate_RTs(extrinsics_a, extrinsics_b, ratio, extrinsics_cameras, num):\n",
    "    R_a, T_a = extrinsics2RT_orig(extrinsics_a)\n",
    "    R_b, T_b = extrinsics2RT_orig(extrinsics_b)\n",
    "    cameras = []\n",
    "    extrapolate_RTs(R_a, T_a, R_b, T_b, ratio, cameras, num)\n",
    "    extrinsics_cameras.extend(RT_orig2extrinsics(r, t) for r, t in cameras)\n",
    "    return extrinsics_cameras[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_line(data_line):\n",
    "    data_line = data_line.strip().split()\n",
    "    intrinsics = np.array(data_line[3:7], dtype=np.float32) / image_size\n",
    "    extrinsics_src = np.array(data_line[7:19], dtype=np.float32)\n",
    "    extrinsics_tgt = np.array(data_line[19:31], dtype=np.float32)\n",
    "    img_src_file = f\"{frame_dir}/{data_line[0]}/{data_line[1]}.png\"\n",
    "    img_tgt_file = f\"{frame_dir}/{data_line[0]}/{data_line[2]}.png\"\n",
    "    img_src = load_image(img_src_file, image_size)\n",
    "    img_tgt = load_image(img_tgt_file, image_size)\n",
    "\n",
    "    return intrinsics, extrinsics_src, extrinsics_tgt, img_src_file, img_tgt_file, img_src, img_tgt\n",
    "\n",
    "def _snap_ours(show=True):\n",
    "    take_pic_ours(img_src, intrinsics, extrinsics_src, extrinsics_new, use_inpainting=use_inpainting, show=show)\n",
    "\n",
    "def _snap_synsin(show=True):\n",
    "    take_synsin_pic(img_src, intrinsics, extrinsics_src, extrinsics_new, show=show)\n",
    "\n",
    "def _snap_im2im(show=True):\n",
    "    take_im2im_pic(img_src, intrinsics, extrinsics_src, extrinsics_new, show=show)\n",
    "\n",
    "def _snap(show=True):\n",
    "    ours_out = take_pic_ours(img_src, intrinsics, extrinsics_src, extrinsics_new, use_inpainting=use_inpainting, show=False)\n",
    "    synsin_out = take_synsin_pic(img_src, intrinsics, extrinsics_src, extrinsics_new, show=False)\n",
    "    if run_im2im:\n",
    "        im2im_out = take_im2im_pic(img_src, intrinsics, extrinsics_src, extrinsics_new, show=False)\n",
    "    else:\n",
    "        im2im_out = np.ones_like(ours_out)\n",
    "\n",
    "    if show:\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(im2im_out)\n",
    "        plt.title(\"Im2Im\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(synsin_out)\n",
    "        plt.title(\"SynSin\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(ours_out)\n",
    "        plt.title(\"ours\")\n",
    "        plt.axis(\"off\")    \n",
    "    else:\n",
    "        return ours_out, synsin_out, im2im_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "intrinsics, extrinsics_src, extrinsics_tgt, img_src_file, img_tgt_file, img_src, img_tgt = load_data_line(\"\"\"\n",
    "024152256b6bcac7 90156733 90757333 121.18203 215.434708 128.0 128.0 0.56202 0.144292 -0.81444 -1.998381 -0.055799 0.989037 0.13672 -0.007332 0.825239 -0.031395 0.56391 -2.192661 0.600657 0.139602 -0.787224 -2.398725 -0.053141 0.989431 0.134914 -0.040777 0.797738 -0.039203 0.601728 -2.26991\n",
    "\"\"\")\n",
    "use_inpainting = False\n",
    "run_im2im = False\n",
    "\n",
    "cameras = []\n",
    "image_key = '_'.join(img_src_file.split('/')[-2:]).replace('.png', ''); print(image_key)\n",
    "extrinsics_new = extrinsics_src\n",
    "\n",
    "for _ in range(90):\n",
    "    cameras.append(extrinsics_new)\n",
    "\n",
    "_snap();\n",
    "extrinsics_new = extrinsics_interpolate_RTs(extrinsics_new, extrinsics_tgt, cameras, num=60); _snap();\n",
    "extrinsics_new = extrinsics_extrapolate_RTs(extrinsics_src, extrinsics_tgt, 1.5, cameras, num=90); _snap();\n",
    "extrinsics_new = extrinsics_move(extrinsics_new, 3, [-1, 0, 0], cameras, sampling=120); _snap();\n",
    "extrinsics_new = extrinsics_move(extrinsics_new, -2, [-1, 0, 0], cameras, sampling=120); _snap();\n",
    "extrinsics_new = extrinsics_h_rotate(extrinsics_new, 15, cameras, sampling=6); _snap();\n",
    "extrinsics_new = extrinsics_h_rotate(extrinsics_new, -30, cameras, sampling=6); _snap();\n",
    "extrinsics_new = extrinsics_h_rotate(extrinsics_new, 15, cameras, sampling=6); _snap();\n",
    "extrinsics_new = extrinsics_interpolate_RTs(extrinsics_new, extrinsics_src, cameras, num=120); _snap();\n",
    "\n",
    "for _ in range(60):\n",
    "    cameras.append(extrinsics_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_inpainting = False\n",
    "run_im2im = False\n",
    "video_file = f\"./save/realestate10k_videos_project_page/{image_key}{'_with_im2im' if run_im2im else ''}.mp4\"\n",
    "os.makedirs(os.path.dirname(video_file), exist_ok=True)\n",
    "\n",
    "import skimage.io\n",
    "from skimage import img_as_float32\n",
    "background_frame = img_as_float32(skimage.io.imread('./tools/scripts/neural_rendering/video_making/data/realestate10k_background_new.png'))\n",
    "\n",
    "frames = []\n",
    "frames_im2im = []\n",
    "frames_synsin = []\n",
    "frames_ours = []\n",
    "\n",
    "pad = np.ones((image_size, image_size // 16, 3), dtype=np.float32)\n",
    "for extrinsics_new in tqdm(cameras):\n",
    "    ours_out, synsin_out, im2im_out = _snap(show=False)\n",
    "    if run_im2im:\n",
    "        concat_out = np.hstack((pad, im2im_out, pad, pad, synsin_out, pad, pad, ours_out, pad))\n",
    "        frames_im2im.append(im2im_out)\n",
    "    else:\n",
    "        concat_out = np.hstack((pad, synsin_out, pad, pad, ours_out, pad))\n",
    "    frames_synsin.append(synsin_out)\n",
    "    frames_ours.append(ours_out)\n",
    "\n",
    "    combined = background_frame.copy()\n",
    "    combined[image_size // 8:concat_out.shape[0] + image_size // 8] = concat_out\n",
    "    frames.append(combined)\n",
    "\n",
    "\n",
    "def write_video(frames, file):\n",
    "    frame_size = (frames[-1].shape[1], frames[-1].shape[0])\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"MP4V\")\n",
    "    fps = 60\n",
    "    writer = cv2.VideoWriter(file, fourcc, fps, frame_size)\n",
    "    for img in frames:\n",
    "        # float32 -> uint8, RGB -> BGR\n",
    "        writer.write(img_as_ubyte(img[..., ::-1]))\n",
    "    writer.release()\n",
    "    \n",
    "\n",
    "write_video(frames, video_file)\n",
    "# if run_im2im:\n",
    "#     write_video(frames_im2im, video_file.replace('.mp4', '_im2im.mp4'))\n",
    "# write_video(frames_synsin, video_file.replace('.mp4', '_synsin.mp4'))\n",
    "# write_video(frames_ours, video_file.replace('.mp4', '_ours.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
